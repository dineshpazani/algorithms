

Spark fault tolerance:
https://techvidvan.com/tutorials/fault-tolerance-in-spark/
https://www.tutorialkart.com/pdf/apache-spark/spark-add-new-column-to-dataset-example.pdf
https://www.dezyre.com/article/hadoop-developer-interview-questions-at-top-tech-companies/243


Spark 3 features:
  Support for python 3


Howmany way creating RDD:
  1. partitions
  2. CSV files and text file or DB
  3. From existing RDD

Spark supporting file format:
  ORC, JSON, parquet ans test file

Spark:
  create RDD:
    create SC (Spark context):
      val intArr = Array(1,2,3,4,5,6,7,8,9,0)
      val rdd = sc.parallelize(intArr)       -> create RDD
      val rdd = sc.parallelize(intArr, 6)    -> create RDD with partitions 
      rdd.take(3)                           -> will return the 3 array of elements 
      rdd.collect()                         -> it will take all elements 
      rdd.collect().foreach(println)        -> it will print all elements in the RDD
      rdd.partitions.size()                 -> to check no of partitions 

    create RDD from file:
      val fileRDD = sc.textFile("file path")
      val fileRDD = sc.textFile("file path", 10) with partiotions 

  RDD Transformations: Lazy evalutaions 

      by RDD:
        val intArr = Array(1,2,3,4,5,6,7,8,9,0)
        val rdd = sc.parallelize(intArr) 
        val rddFilter = rdd.filter(line => line.split(","))

      by Map: return colection of collections 
        val intArr = Array(1,2,3,4,5,6,7,8,9,0)
        val rdd = sc.parallelize(intArr) 
        val rddFilter = rdd.map(line => line.split(","))

      by Float Map: return float of collection
        val intArr = Array(1,2,3,4,5,6,7,8,9,0)
        val rdd = sc.parallelize(intArr) 
        val rddFilter = rdd.FloatMap(line => line.split(","))


  Load CSV files in RDD:
    var csvRDD = new SparkContext(sparkconfig).textFile("src/test/resources/train.csv");

    csvRDD.take(10).foreach(println)

  //with out header:
    var header = csvRDD.first()    
    var csvRddWithoutHeader = csvRDD.filter(_ != header)    
    csvRddWithoutHeader.foreach(println)
    
   
  // csv take particular column to rdd:
   var csvArray = csvRddWithoutHeader.map(line =>{
     
     var csvArray = line.split(" ")
     
     if(csvArray.length>4)
       Array(csvArray(0), csvArray(1), csvArray(3)).mkString(" ")
      else
        println(csvArray(0))
     
   }).take(10).foreach(println)
  //csvArray.foreach(println)


  //Save file in csv:
    rdd.saveAsTextFile(path)


  creating Schema:



RDD Support action:
  Transformations -> One RDD to another RDD. Map and filter
  Actions -> reduce and take -> repeat the value untill getting final result.

Spark code:

  memory management
  Job scheduling
  monitoring the jobs,
  providing fault tolerance.

Hive on Spark:

  hive> set spark.home=/location/to/sparkHome;
hive> set hive.execution.engine=spark;

Hive supports Spark on YARN mode by default.

Spark eco system:
  Spark sql,
  Spark streaming
  Graphx
  MLlib,
  SparkR


YARN:

  Yarn is venral ans resource management platform.

Cluster Managers:

  Standalone
  Apache mesos
  YARN


Broadcast and Accumulators:
    Using Broadcast Variable- Broadcast variable enhances the efficiency of joins between small and large RDDs.
    Using Accumulators â€“ Accumulators help update the values of variables in parallel while executing.
    The most common way is to avoid operations ByKey, repartition or any other operations which trigger shuffles.




Site 1:
  https://data-flair.training/blogs/apache-spark-interview-questions-answers/
  https://data-flair.training/blogs/apache-spark-interview-questions/
  https://data-flair.training/blogs/apache-spark-interview-questions-and-answers/




Data set:

  Dataset[T] -> Dynamically typed collection

  Typed transfermation - return data set
  Untyped transfermation - return data frame

  Strongly typed
  Seriazatied and deserialized
  2x faster ten data frame
  does not supporting python and R



